@misc{andersonMeasuringMitigatingHallucinations2025,
  title = {Measuring and {{Mitigating Hallucinations}} in {{Vision-Language Dataset Generation}} for {{Remote Sensing}}},
  author = {Anderson, Madeline and Cha, Miriam and Freeman, William T. and Perron, J. Taylor and Maidel, Nathaniel and Cahoy, Kerri},
  year = 2025,
  month = jan,
  number = {arXiv:2501.14905},
  eprint = {2501.14905},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14905},
  urldate = {2025-09-16},
  abstract = {Vision language models have achieved impressive results across various fields. However, adoption in remote sensing remains limited, largely due to the scarcity of paired image-text data. To bridge this gap, synthetic caption generation has gained interest, traditionally relying on rule-based methods that use metadata or bounding boxes. While these approaches provide some description, they often lack the depth needed to capture complex wide-area scenes. Large language models (LLMs) offer a promising alternative for generating more descriptive captions, yet they can produce generic outputs and are prone to hallucination. In this paper, we propose a new method to enhance vision-language datasets for remote sensing by integrating maps as external data sources, enabling the generation of detailed, context-rich captions. Additionally, we present methods to measure and mitigate hallucinations in LLM-generated text. We introduce fMoW-mm, a multimodal dataset incorporating satellite imagery, maps, metadata, and text annotations. We demonstrate its effectiveness for automatic target recognition in few-shot settings, achieving superior performance compared to other vision-language remote sensing datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/YQGPEITD/Anderson et al. - 2025 - Measuring and Mitigating Hallucinations in Vision-Language Dataset Generation for Remote Sensing.pdf;/home/filipe/Zotero/storage/YMIZC7TS/2501.html}
}

@misc{baiQwen3VLTechnicalReport2025,
  title = {Qwen3-{{VL Technical Report}}},
  author = {Bai, Shuai and Cai, Yuxuan and Chen, Ruizhe and Chen, Keqin and Chen, Xionghui and Cheng, Zesen and Deng, Lianghao and Ding, Wei and Gao, Chang and Ge, Chunjiang and Ge, Wenbin and Guo, Zhifang and Huang, Qidong and Huang, Jie and Huang, Fei and Hui, Binyuan and Jiang, Shutong and Li, Zhaohai and Li, Mingsheng and Li, Mei and Li, Kaixin and Lin, Zicheng and Lin, Junyang and Liu, Xuejing and Liu, Jiawei and Liu, Chenglong and Liu, Yang and Liu, Dayiheng and Liu, Shixuan and Lu, Dunjie and Luo, Ruilin and Lv, Chenxu and Men, Rui and Meng, Lingchen and Ren, Xuancheng and Ren, Xingzhang and Song, Sibo and Sun, Yuchong and Tang, Jun and Tu, Jianhong and Wan, Jianqiang and Wang, Peng and Wang, Pengfei and Wang, Qiuyue and Wang, Yuxuan and Xie, Tianbao and Xu, Yiheng and Xu, Haiyang and Xu, Jin and Yang, Zhibo and Yang, Mingkun and Yang, Jianxin and Yang, An and Yu, Bowen and Zhang, Fei and Zhang, Hang and Zhang, Xi and Zheng, Bo and Zhong, Humen and Zhou, Jingren and Zhou, Fan and Zhou, Jing and Zhu, Yuanzhi and Zhu, Ke},
  year = 2025,
  month = nov,
  number = {arXiv:2511.21631},
  eprint = {2511.21631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.21631},
  urldate = {2026-01-01},
  abstract = {We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/ATS8QBLI/Bai et al. - 2025 - Qwen3-VL Technical Report.pdf;/home/filipe/Zotero/storage/5886L3KS/2511.html}
}

@inproceedings{basakAerialMirageUnmasking2025a,
  title = {Aerial {{Mirage}}: {{Unmasking Hallucinations}} in {{Large Vision Language Models}}},
  shorttitle = {Aerial {{Mirage}}},
  booktitle = {2025 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Basak, Debolena and Bhatt, Soham and Kanduri, Sahith and Desarkar, Maunendra Sankar},
  year = 2025,
  month = feb,
  pages = {5500--5508},
  issn = {2642-9381},
  doi = {10.1109/WACV61041.2025.00537},
  urldate = {2026-01-14},
  abstract = {Drones excel at capturing aerial-view images, especially in human unreachable areas. Automatically interpreting and describing these images enables decision-making easier without the need to review the images extensively. Traditional image captioning models struggle with aerial imagery due to diverse orientations, perspectives, and unclear objects. Integrating the capabilities of Large Vision Language Models (LVLMs) with drone images can improve description utility, benefiting strategic missions like surveillance, search and rescue, etc. However, the lack of image-caption datasets for drone imagery poses a significant challenge for training and evaluating drone image captioning. To address this gap, we contribute the first Aerial-view Image Captioning dataset (AeroCaps), containing four captions per image. Another major hurdle for the task is the hallucinatory nature of LVLMs. To this end, we perform the first extensive analysis of hallucinations on aerial imagery by two SOTA LVLMs - LLaVA and InstructBLIP on our proposed dataset and VisDrone. We explore the reasons behind such hallucinations. We release the LVLM-generated image captions along with our hallucination-labelled annotations as the Labelled Illusion Dataset (LID) for further research. Additionally, we review how effective advanced LLMs like GPT-4 are in evaluating the degree of hallucinations made by other LVLMs like LLaVA.},
  keywords = {Annotations,Computational modeling,Computer vision,Data models,Decision making,Drones,Reliability,Reviews,Surveillance,Training},
  file = {/home/filipe/Zotero/storage/HM7K95EX/Basak et al. - 2025 - Aerial Mirage Unmasking Hallucinations in Large Vision Language Models.pdf}
}

@article{chenDBBlendMaskDecomposedAttention2022,
  title = {{{DB-BlendMask}}: {{Decomposed Attention}} and {{Balanced BlendMask}} for {{Instance Segmentation}} of {{High-Resolution Remote Sensing Images}}},
  shorttitle = {{{DB-BlendMask}}},
  author = {Chen, Zhenqian and Shang, Yongheng and Python, Andre and Cai, Yuxiang and Yin, Jianwei},
  year = 2022,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {60},
  pages = {1--15},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2021.3138913},
  urldate = {2026-01-12},
  abstract = {Instance segmentation is an important method for high-resolution remote sensing images (HRRSIs) analysis. Traditional instance segmentation algorithms are not suitable to analyze complex HRRSIs that exhibit: 1) various shapes and sizes of targets; 2) a large number of small targets; and 3) data with long tail distribution. Here we introduce DB-BlendMask, an efficient and accurate instance segmentation method that can accommodate complex HRRSIs. It is composed of size balance coefficient (SBC), class balance module (CBM), and decomposed attention blender module (DA-Blender module). SBC consists of a fair weight allocation strategy for positive samples in object detection. CBM combines classification obtained in object detection stage to guide the semantic feature extraction. Complementary to a traditional convolutional neural network (CNN) architecture, DA-Blender module has the ability to considerably compress space complexity of attention and merge attention with semantic feature to generate the instance mask. We compare the performance of DB-BlendMask with a benchmark Mask R-CNN on two typical datasets, iSAID, and ISPRS Postdam. We obtain an average detection precision of 39.2\% on iSAID and 63.6\% on ISPRS Postdam, which corresponds to an improvement of 2.5\% and 2.7\%, respectively, compared to the benchmark in a real-time scenario.},
  keywords = {BlendMask,class balance module (CBM),decomposed attention blender module (DA-Blender module),Feature extraction,high-resolution remote sensing images (HRRSIs),Image segmentation,instance segmentation,Marine vehicles,Object detection,Resource management,Semantics,Shape,size balance coefficient (SBC)},
  file = {/home/filipe/Zotero/storage/D64EXASJ/Chen et al. - 2022 - DB-BlendMask Decomposed Attention and Balanced BlendMask for Instance Segmentation of High-Resoluti.pdf}
}

@article{farquharDetectingHallucinationsLarge2024,
  title = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  year = 2024,
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07421-0},
  urldate = {2025-09-18},
  abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often `hallucinate' false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has~been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations---confabulations---which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Information technology},
  file = {/home/filipe/Zotero/storage/HIKAMJHU/Farquhar et al. - 2024 - Detecting hallucinations in large language models using semantic entropy.pdf}
}

@inproceedings{filippovaControlledHallucinationsLearning2020,
  title = {Controlled {{Hallucinations}}: {{Learning}} to {{Generate Faithfully}} from {{Noisy Data}}},
  shorttitle = {Controlled {{Hallucinations}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Filippova, Katja},
  editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
  year = 2020,
  month = nov,
  pages = {864--870},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.76},
  urldate = {2026-02-02},
  abstract = {Neural text generation (data- or text-to-text) demonstrates remarkable performance when training data is abundant which for many applications is not the case. To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input. Consequently, models pick up on the noise and may hallucinate--generate fluent but unsupported text. Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus (Lebret et al., 2016), a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation.},
  file = {/home/filipe/Zotero/storage/IEK92MAL/Filippova - 2020 - Controlled Hallucinations Learning to Generate Faithfully from Noisy Data.pdf}
}

@article{geRSTellerScalingVisual2025,
  title = {{{RSTeller}}: {{Scaling Up Visual Language Modeling}} in {{Remote Sensing}} with {{Rich Linguistic Semantics}} from {{Openly Available Data}} and {{Large Language Models}}},
  shorttitle = {{{RSTeller}}},
  author = {Ge, Junyao and Zhang, Xu and Zheng, Yang and Guo, Kaitai and Liang, Jimin},
  year = 2025,
  month = aug,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {226},
  eprint = {2408.14744},
  primaryclass = {cs},
  pages = {146--163},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2025.05.002},
  urldate = {2026-01-01},
  abstract = {Abundant, well-annotated multimodal data in remote sensing are pivotal for aligning complex visual remote sensing (RS) scenes with human language, enabling the development of specialized vision language models across diverse RS interpretation tasks. However, annotating RS images with rich linguistic semantics at scale demands expertise in RS and substantial human labor, making it costly and often impractical. In this study, we propose a workflow that leverages large language models (LLMs) to generate multimodal datasets with semantically rich captions at scale from plain OpenStreetMap (OSM) data for images sourced from the Google Earth Engine (GEE) platform. This approach facilitates the generation of paired remote sensing data and can be readily scaled up using openly available data. Within this framework, we present RSTeller, a multimodal dataset comprising over 1.3 million RS images, each accompanied by two descriptive captions. Extensive experiments demonstrate that RSTeller enhances the performance of multiple existing vision language models for RS scene understanding through continual pre-training. Our methodology significantly reduces the manual effort and expertise needed for annotating remote sensing imagery while democratizing access to high-quality annotated data. This advancement fosters progress in visual language modeling and encourages broader participation in remote sensing research and applications. The RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/5GVHEPSB/Ge et al. - 2025 - RSTeller Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from.pdf;/home/filipe/Zotero/storage/ZWRR5D8G/2408.html}
}

@misc{grattafioriLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzm{\'a}n, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and {Rantala-Yeary}, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, V{\'i}tor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  year = 2024,
  month = nov,
  number = {arXiv:2407.21783},
  eprint = {2407.21783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.21783},
  urldate = {2025-11-16},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/RVLNJ9A5/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf;/home/filipe/Zotero/storage/FCEIJXF2/2407.html}
}

@article{gunjalDetectingPreventingHallucinations2024,
  title = {Detecting and {{Preventing Hallucinations}} in {{Large Vision Language Models}}},
  author = {Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  year = 2024,
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {18135--18143},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i16.29771},
  urldate = {2025-09-19},
  abstract = {Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41\% and 55\% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15\% and 57\% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {NLP: Safety and Robustness},
  file = {/home/filipe/Zotero/storage/49FKPMUZ/Gunjal et al. - 2024 - Detecting and Preventing Hallucinations in Large Vision Language Models.pdf}
}

@inproceedings{ICLR2024_fc625e83,
  title = {Analyzing and Mitigating Object Hallucination in Large Vision-Language Models},
  booktitle = {International Conference on Representation Learning},
  author = {Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  editor = {Kim, B. and Yue, Y. and Chaudhuri, S. and Fragkiadaki, K. and Khan, M. and Sun, Y.},
  date = {2024},
  volume = {2024},
  pages = {56969--56998},
  url = {https://proceedings.iclr.cc/paper_files/paper/2024/file/fc625e831361cfcc82cb74224fdc66cb-Paper-Conference.pdf},
  file = {/home/filipe/Zotero/storage/PGWQYICI/Zhou et al. - 2024 - Analyzing and mitigating object hallucination in large vision-language models.pdf}
}

@article{jiSurveyHallucinationNatural2023,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  year = 2023,
  month = mar,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {12},
  pages = {248:1--248:38},
  issn = {0360-0300},
  doi = {10.1145/3571730},
  urldate = {2026-02-04},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
  file = {/home/filipe/Zotero/storage/JHDGN7P3/Ji et al. - 2023 - Survey of Hallucination in Natural Language Generation.pdf}
}

@misc{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  year = 2023,
  month = apr,
  number = {arXiv:2304.02643},
  eprint = {2304.02643},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.02643},
  urldate = {2025-09-25},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/filipe/Zotero/storage/W5BSW7IC/Kirillov et al. - 2023 - Segment Anything.pdf;/home/filipe/Zotero/storage/H9DZFCJR/2304.html}
}

@inproceedings{kuckrejaGeoChatGroundedLarge2024,
  title = {{{GeoChat}}: {{Grounded Large Vision-Language Model}} for {{Remote Sensing}}},
  shorttitle = {{{GeoChat}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kuckreja, Kartik and Danish, Muhammad Sohail and Naseer, Muzammal and Das, Abhijit and Khan, Salman and Khan, Fahad Shahbaz},
  year = 2024,
  pages = {27831--27840},
  urldate = {2026-01-14},
  langid = {english},
  file = {/home/filipe/Zotero/storage/THXA8FFV/Kuckreja et al. - 2024 - GeoChat Grounded Large Vision-Language Model for Remote Sensing.pdf}
}

@inproceedings{lengMitigatingObjectHallucinations2024,
  title = {Mitigating {{Object Hallucinations}} in {{Large Vision-Language Models}} through {{Visual Contrastive Decoding}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  year = 2024,
  month = jun,
  pages = {13872--13882},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.01316},
  urldate = {2025-10-20},
  abstract = {Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  file = {/home/filipe/Zotero/storage/6UU8QDRK/Leng et al. - 2024 - Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.pdf}
}

@inproceedings{liEvaluatingObjectHallucination2023a,
  title = {Evaluating {{Object Hallucination}} in {{Large Vision-Language Models}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Xin and Wen, Ji-Rong},
  year = 2023,
  pages = {292--305},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.20},
  urldate = {2026-01-14},
  langid = {english},
  file = {/home/filipe/Zotero/storage/BIZUD6L2/Li et al. - 2023 - Evaluating Object Hallucination in Large Vision-Language Models.pdf}
}

@article{liInsightAnyInstance2025,
  title = {Insight {{Any Instance}}: {{Promptable Instance Segmentation}} for {{Remote Sensing Images}}},
  shorttitle = {Insight {{Any Instance}}},
  author = {Li, Xuexue and Diao, Wenhui and Li, Xinming and Sun, Xian},
  year = 2025,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {63},
  pages = {1--15},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2025.3543636},
  urldate = {2025-09-25},
  abstract = {Instance segmentation of remote sensing images (RSIs) is an essential task for a wide range of applications such as land planning and intelligent transport. Instance segmentation of RSIs is constantly plagued by the unbalanced ratio of foreground and background and limited instance size. And most of the instance segmentation models are based on deep feature learning and contain operations such as multiple downsampling, which is harmful to instance segmentation of RSIs, and thus the performance is still limited. Inspired by the recent superior performance of prompt learning in visual tasks, we propose a new prompt paradigm to address the above issues. Based on the existing instance segmentation model, first, a local prompt module is designed to mine local prompt information from original local tokens for specific instances; second, a global-to-local prompt module is designed to model the contextual information from the global tokens to the local tokens where the instances are located for specific instances. Finally, a proposal's area loss function (PAreaLoss) is designed to add a decoupling dimension for proposals on the scale to better exploit the potential of the above two prompt modules. It is worth mentioning that our proposed approach can extend the instance segmentation model to a promptable instance segmentation model, i.e., to segment the instances with the specific boxes' prompt. The time consumption for each promptable instance segmentation process is only 40 ms. This article evaluates the effectiveness of our proposed approach based on several existing models in four instance segmentation datasets of RSIs, and thorough experiments prove that our proposed approach is effective for addressing the above issues and is a competitive model for instance segmentation of RSIs.},
  keywords = {Computational modeling,Context modeling,Data models,Feature extraction,Global-to-local,instance segmentation,Instance segmentation,prompt,Proposals,remote sensing,Remote sensing,Representation learning,Transformers,Visualization},
  file = {/home/filipe/Zotero/storage/SCMHV7CN/Li et al. - 2025 - Insight Any Instance Promptable Instance Segmentation for Remote Sensing Images.pdf}
}

@misc{liuImprovedBaselinesVisual2024a,
  title = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  year = 2024,
  month = may,
  number = {arXiv:2310.03744},
  eprint = {2310.03744},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03744},
  urldate = {2026-01-01},
  abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in \textasciitilde 1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/filipe/Zotero/storage/P342B9W9/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf;/home/filipe/Zotero/storage/YJ2ZYRZ9/2310.html}
}

@misc{liuSurveyHallucinationLarge2024,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Vision-Language Models}}},
  author = {Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
  year = 2024,
  month = may,
  number = {arXiv:2402.00253},
  eprint = {2402.00253},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.00253},
  urldate = {2025-10-20},
  abstract = {Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/filipe/Zotero/storage/LK2F52ZC/Liu et al. - 2024 - A Survey on Hallucination in Large Vision-Language Models.pdf;/home/filipe/Zotero/storage/MRRYVFGQ/2402.html}
}

@misc{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = 2023,
  month = dec,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08485},
  urldate = {2025-12-11},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/filipe/Zotero/storage/QBR4IKJ7/Liu et al. - 2023 - Visual Instruction Tuning.pdf;/home/filipe/Zotero/storage/YJITK5Y4/2304.html}
}

@article{luoSAMRSISProgressivelyAdapting2024,
  title = {{{SAM-RSIS}}: {{Progressively Adapting SAM With Box Prompting}} to {{Remote Sensing Image Instance Segmentation}}},
  shorttitle = {{{SAM-RSIS}}},
  author = {Luo, Muying and Zhang, Tao and Wei, Shiqing and Ji, Shunping},
  year = 2024,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {62},
  pages = {1--14},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2024.3460085},
  urldate = {2025-09-25},
  abstract = {The recent segment anything model (SAM) trained on massive close-range images has demonstrated impressive performance on general segmentation or specific segmentation tasks with manual prompts. However, the significant domain shift problem between remote sensing and close-range images should be tackled before introducing the pretrained SAM to remote sensing instance segmentation (RSIS). To address this and unlock the potential of SAM in RSIS, this article proposes a novel framework called SAM for remote sensing instance segmentation (SAM-RSIS), which overcomes the problems in a few recent works that only adapt a part of SAM to remote sensing. SAM-RSIS fine-tunes the vision transformer (ViT) backbone and mask decoder of SAM progressively on remote sensing data and uses automatic box prompting to eliminate the need for manual prompting. SAM-RSIS consists of an object detection stage and a mask generation stage. In object detection, we introduce an adapter to adapt knowledge embedded in the pretrained ViT backbone to remote sensing images and then build an object detector. In mask generation, using the detected bounding boxes as prompts, along with two learnable mask output tokens, and the two-layer high-resolution features from the adapter, we fine-tune the mask decoder of SAM to produce high-quality masks. Experimental results on the WHU, WHU-Mix, and NWPU datasets for binary and multiclass RSIS demonstrate the effectiveness and robustness of the proposed method, surpassing various derivative methods of SAM and achieving performance comparable to and even better than the specific state-of-the-art instance segmentation methods.},
  keywords = {Adaptation models,Decoding,Efficient tuning,foundation model,Image segmentation,instance segmentation,Instance segmentation,Object detection,remote sensing,Remote sensing,segment anything model (SAM),Semantics},
  file = {/home/filipe/Zotero/storage/9SUXMPS8/Luo et al. - 2024 - SAM-RSIS Progressively Adapting SAM With Box Prompting to Remote Sensing Image Instance Segmentatio.pdf}
}

@inproceedings{marimoVisibleMultispectralVisionLanguage2026,
  title = {Beyond the~{{Visible}}: {{Multispectral Vision-Language Learning}} for~{{Earth Observation}}},
  shorttitle = {Beyond the~{{Visible}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {Marimo, Clive Tinashe and Blumenstiel, Benedikt and Nitsche, Maximilian and Jakubik, Johannes and Brunschwiler, Thomas},
  editor = {Ribeiro, Rita P. and Pfahringer, Bernhard and Japkowicz, Nathalie and Larra{\~n}aga, Pedro and Jorge, Al{\'i}pio M. and Soares, Carlos and Abreu, Pedro H. and Gama, Jo{\~a}o},
  year = 2026,
  pages = {359--375},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-032-06106-5_21},
  abstract = {Vision-language models for Earth observation (EO) typically rely on the visual spectrum of data as the only model input, thus failing to leverage the rich spectral information available in the multispectral channels recorded by satellites. Therefore, we introduce Llama3-MS-CLIP----the first vision-language model pre-trained with contrastive learning on a large-scale multispectral dataset and report on the performance gains due to the extended spectral range. Furthermore, we present the largest-to-date image-caption dataset for multispectral data, consisting of one million Sentinel-2 samples and corresponding textual descriptions generated using Llama3-LLaVA-Next and Overture Maps data. We develop a scalable captioning pipeline, which is validated by domain experts. We evaluate Llama3-MS-CLIP on multispectral zero-shot image classification and retrieval using three datasets of varying complexity. Our results demonstrate that Llama3-MS-CLIP significantly outperforms other RGB-based approaches, improving classification accuracy by +6.77\% on average and retrieval performance by +4.63\% mAP compared to the second-best model. Our results emphasize the relevance of multispectral vision-language learning. The image-caption dataset, code, and model weights are available at https://github.com/IBM/MS-CLIP.},
  isbn = {978-3-032-06106-5},
  langid = {english},
  keywords = {Earth Observation,Multispectral Data,Vision-Language Model},
  file = {/home/filipe/Zotero/storage/BPR6NQKQ/Marimo et al. - 2026 - Beyond the Visible Multispectral Vision-Language Learning for Earth Observation.pdf}
}

@inproceedings{NEURIPS2024_4ea4a1ea,
  title = {Multi-Object Hallucination in Vision Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Xuweiyi and Ma, Ziqiao and Zhang, Xuejun and Xu, Sihan and Yang, Jianing and Fouhey, David F. and Chai, Joyce and Qian, Shengyi},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = 2024,
  volume = {37},
  pages = {44393--44418},
  publisher = {Curran Associates, Inc.},
  doi = {10.52202/079017-1409},
  file = {/home/filipe/Zotero/storage/3X75938Z/Chen et al. - 2024 - Multi-object hallucination in vision language models.pdf}
}

@inproceedings{NEURIPS2024_c1e1ad23,
  title = {Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hu, Jian and Lin, Jiayi and Yan, Junchi and Gong, Shaogang},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = 2024,
  volume = {37},
  pages = {107171--107197},
  publisher = {Curran Associates, Inc.},
  doi = {10.52202/079017-3403},
  file = {/home/filipe/Zotero/storage/84NBZ4QA/Hu et al. - 2024 - Leveraging hallucinations to reduce manual prompt dependency in promptable segmentation.pdf}
}

@inproceedings{papineniBLEUMethodAutomatic2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = 2002,
  month = jul,
  series = {{{ACL}} '02},
  pages = {311--318},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/1073083.1073135},
  urldate = {2026-01-22},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
  file = {/home/filipe/Zotero/storage/G2FKQE4J/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf}
}

@misc{parkHalLocTokenlevelLocalization2025,
  title = {{{HalLoc}}: {{Token-level Localization}} of {{Hallucinations}} for {{Vision Language Models}}},
  shorttitle = {{{HalLoc}}},
  author = {Park, Eunkyu and Kim, Minyeong and Kim, Gunhee},
  year = 2025,
  month = jun,
  number = {arXiv:2506.10286},
  eprint = {2506.10286},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.10286},
  urldate = {2025-07-04},
  abstract = {Hallucinations pose a significant challenge to the reliability of large vision-language models, making their detection essential for ensuring accuracy in critical applications. Current detection methods often rely on computationally intensive models, leading to high latency and resource demands. Their definitive outcomes also fail to account for real-world scenarios where the line between hallucinated and truthful information is unclear. To address these issues, we propose HalLoc, a dataset designed for efficient, probabilistic hallucination detection. It features 150K token-level annotated samples, including hallucination types, across Visual Question Answering (VQA), instruction-following, and image captioning tasks. This dataset facilitates the development of models that detect hallucinations with graded confidence, enabling more informed user interactions. Additionally, we introduce a baseline model trained on HalLoc, offering low-overhead, concurrent hallucination detection during generation. The model can be seamlessly integrated into existing VLMs, improving reliability while preserving efficiency. The prospect of a robust plug-and-play hallucination detection module opens new avenues for enhancing the trustworthiness of vision-language models in realworld applications. The HalLoc dataset and code are publicly available at: https://github.com/dbsltm/ cvpr25\_halloc.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/W6BAI7LJ/Park et al. - 2025 - HalLoc Token-level Localization of Hallucinations for Vision Language Models.pdf}
}

@article{sumbulBigEarthNetMMLargeScaleMultimodal2021,
  title = {{{BigEarthNet-MM}}: {{A Large-Scale}}, {{Multimodal}}, {{Multilabel Benchmark Archive}} for {{Remote Sensing Image Classification}} and {{Retrieval}} [{{Software}} and {{Data Sets}}]},
  shorttitle = {{{BigEarthNet-MM}}},
  author = {Sumbul, Gencer and {de Wall}, Arne and Kreuziger, Tristan and Marcelino, Filipe and Costa, Hugo and Benevides, Pedro and Caetano, M{\'a}rio and Demir, Beg{\"u}m and Markl, Volker},
  year = 2021,
  month = sep,
  journal = {IEEE Geoscience and Remote Sensing Magazine},
  volume = {9},
  number = {3},
  pages = {174--180},
  issn = {2168-6831},
  doi = {10.1109/MGRS.2021.3089174},
  urldate = {2025-11-27},
  abstract = {This article presents the multimodal BigEarthNet (BigEarthNet-MM) benchmark archive consisting of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to support deep learning (DL) studies in multimodal, multilabel remote sensing (RS) image retrieval and classification. Each pair of patches in BigEarthNet-MM is annotated with multilabels provided by the CORINE Land Cover (CLC) map of 2018 based on its thematically most detailed level-3 class nomenclature. Our initial research demonstrates that some CLC classes are challenging to accurately describe by considering only (single-date) BigEarthNet-MM images. In this article, we also introduce an alternative class nomenclature as an evolution of the original CLC labels to address this problem. This is achieved by interpreting and arranging the CLC level-3 nomenclature based on the properties of BigEarthNet-MM images in a new nomenclature of 19 classes. In our experiments, we show the potential of BigEarthNet-MM for multimodal, multilabel image retrieval and classification problems by considering several state-of-the-art DL models.},
  keywords = {Benchmark testing,Deep learning,Image retrieval,Multimodal sensors,Remote sensing},
  file = {/home/filipe/Zotero/storage/6MCFHY78/Sumbul et al. - 2021 - BigEarthNet-MM A Large-Scale, Multimodal, Multilabel Benchmark Archive for Remote Sensing Image Cla.pdf;/home/filipe/Zotero/storage/U9EHNTBX/9552024.html}
}

@misc{sureshNoiseNarrativeTracing2025,
  title = {From {{Noise}} to {{Narrative}}: {{Tracing}} the {{Origins}} of {{Hallucinations}} in {{Transformers}}},
  shorttitle = {From {{Noise}} to {{Narrative}}},
  author = {Suresh, Praneet and Stanley, Jack and Joseph, Sonia and Scimeca, Luca and Bzdok, Danilo},
  year = 2025,
  month = sep,
  number = {arXiv:2509.06938},
  eprint = {2509.06938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.06938},
  urldate = {2025-10-20},
  abstract = {As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/filipe/Zotero/storage/TVMNT7HB/Suresh et al. - 2025 - From Noise to Narrative Tracing the Origins of Hallucinations in Transformers.pdf;/home/filipe/Zotero/storage/L8ZXLSJW/2509.html}
}

@inproceedings{tangSeeingFarClearly2025b,
  title = {Seeing {{Far}} and {{Clearly}}: {{Mitigating Hallucinations}} in {{MLLMs}} with {{Attention Causal Decoding}}},
  shorttitle = {Seeing {{Far}} and {{Clearly}}},
  booktitle = {2025 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tang, Feilong and Liu, Chengzhi and Xu, Zhongxing and Hu, Ming and Huang, Zile and Xue, Haochen and Chen, Ziyang and Peng, Zelin and Yang, Zhiwei and Zhou, Sijin and Li, Wenxue and Li, Yulong and Song, Wenxuan and Su, Shiyan and Feng, Wei and Su, Jionglong and Lin, Minquan and Peng, Yifan and Cheng, Xuelian and Razzak, Imran and Ge, Zongyuan},
  year = 2025,
  month = jun,
  pages = {26147--26159},
  issn = {2575-7075},
  doi = {10.1109/CVPR52734.2025.02435},
  urldate = {2026-01-14},
  abstract = {Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks.},
  keywords = {Data mining,Decoding,Encoding,FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks,Heart,Interference,Large language models,proving its effectiveness,Question answering (information retrieval),Registers,Video sequences,Visualization,With extensive experiments},
  file = {/home/filipe/Zotero/storage/Q7DAFX39/Tang et al. - 2025 - Seeing Far and Clearly Mitigating Hallucinations in MLLMs with Attention Causal Decoding.pdf}
}

@inproceedings{vedantamCIDErConsensusBasedImage2015,
  title = {{{CIDEr}}: {{Consensus-Based Image Description Evaluation}}},
  shorttitle = {{{CIDEr}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Vedantam, Ramakrishna and Lawrence Zitnick, C. and Parikh, Devi},
  year = 2015,
  pages = {4566--4575},
  urldate = {2026-01-22},
  file = {/home/filipe/Zotero/storage/EXZH9E9J/Vedantam et al. - 2015 - CIDEr Consensus-Based Image Description Evaluation.pdf}
}

@article{yuanChatEarthNetGlobalscaleImage2025,
  title = {{{ChatEarthNet}}: A Global-Scale Image--Text Dataset Empowering Vision--Language Geo-Foundation Models},
  shorttitle = {{{ChatEarthNet}}},
  author = {Yuan, Zhenghang and Xiong, Zhitong and Mou, Lichao and Zhu, Xiao Xiang},
  year = 2025,
  month = mar,
  journal = {Earth System Science Data},
  volume = {17},
  number = {3},
  pages = {1245--1263},
  issn = {1866-3516},
  doi = {10.5194/essd-17-1245-2025},
  urldate = {2026-01-14},
  abstract = {Abstract. The rapid development of remote sensing technology has led to an exponential growth in satellite images, yet their inherent complexity often makes them difficult for non-expert users to understand. Natural language, as a carrier of human knowledge, can bridge the gap between common users and complicated satellite imagery. Additionally, when paired with visual data, natural language can be utilized to train large vision--language foundation models, significantly improving performance in various tasks. Despite these advancements, the remote sensing community still faces a challenge due to the lack of large-scale, high-quality vision--language datasets for satellite images. To address this challenge, we introduce a new image--text dataset, providing high-quality natural language descriptions for global-scale satellite data. Specifically, we utilize Sentinel-2 data for its global coverage as the foundational image source, employing semantic segmentation labels from the European Space Agency's WorldCover project to enrich the descriptions of land cover types. By conducting in-depth semantic analysis, we formulate detailed prompts to elicit rich descriptions from ChatGPT. We then include a manual verification process to enhance the dataset's quality further. This step involves manual inspection and correction to refine the dataset. Finally, we offer the community ChatEarthNet, a large-scale image--text dataset characterized by global coverage, high quality, wide-ranging diversity, and detailed descriptions. ChatEarthNet consists of 163\,488 image--text pairs with captions generated by ChatGPT-3.5 and an additional 10\,000 image--text pairs with captions generated by ChatGPT-4V(ision). This dataset has significant potential for both training and evaluating vision--language geo-foundation models for remote sensing. The code is publicly available at~https://doi.org/10.5281/zenodo.11004358~(Yuan et~al.,~2024b), and the ChatEarthNet dataset is available at~https://doi.org/10.5281/zenodo.11003436~(Yuan et~al.,~2024c).},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/filipe/Zotero/storage/PQ3WSU3Z/Yuan et al. - 2025 - ChatEarthNet a global-scale image–text dataset empowering vision–language geo-foundation models.pdf}
}

@misc{zavrasGAIAGlobalMultimodal2025,
  title = {{{GAIA}}: {{A Global}}, {{Multi-modal}}, {{Multi-scale Vision-Language Dataset}} for {{Remote Sensing Image Analysis}}},
  shorttitle = {{{GAIA}}},
  author = {Zavras, Angelos and Michail, Dimitrios and Zhu, Xiao Xiang and Demir, Beg{\"u}m and Papoutsis, Ioannis},
  year = 2025,
  month = feb,
  number = {arXiv:2502.09598},
  eprint = {2502.09598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.09598},
  urldate = {2025-10-27},
  abstract = {The continuous operation of Earth-orbiting satellites generates vast and ever-growing archives of Remote Sensing (RS) images. Natural language presents an intuitive interface for accessing, querying, and interpreting the data from such archives. However, existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/filipe/Zotero/storage/TEGAU77I/Zavras et al. - 2025 - GAIA A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis.pdf;/home/filipe/Zotero/storage/MPSZ4PQJ/2502.html}
}

@article{zhangRS5MGeoRSCLIPLarge2024,
  title = {{{RS5M}} and {{GeoRSCLIP}}: {{A Large Scale Vision-Language Dataset}} and {{A Large Vision-Language Model}} for {{Remote Sensing}}},
  shorttitle = {{{RS5M}} and {{GeoRSCLIP}}},
  author = {Zhang, Zilun and Zhao, Tiancheng and Guo, Yulong and Yin, Jianwei},
  year = 2024,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {62},
  eprint = {2306.11300},
  primaryclass = {cs},
  pages = {1--23},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2024.3449154},
  urldate = {2026-01-01},
  abstract = {Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap between the General Vision-Language Model (GVLM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning methods on RS5M to implement the DVLM. Experimental results show that our proposed dataset is highly effective for various tasks, and our model GeoRSCLIP improves upon the baseline or previous state-of-the-art model by \$3\textbackslash\%\textbackslash sim20\textbackslash\%\$ in Zero-shot Classification (ZSC), \$3\textbackslash\%\textbackslash sim6\textbackslash\%\$ in Remote Sensing Cross-Modal Text-Image Retrieval (RSCTIR) and \$4\textbackslash\%\textbackslash sim5\textbackslash\%\$ in Semantic Localization (SeLo) tasks. Dataset and models have been released in: \textbackslash url\textbraceleft https://github.com/om-ai-lab/RS5M\textbraceright.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {/home/filipe/Zotero/storage/SSK59KBY/2306.html}
}

@inproceedings{maynezFaithfulnessFactualityAbstractive2020,
  title = {On {{Faithfulness}} and {{Factuality}} in {{Abstractive Summarization}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = 2020,
  month = jul,
  pages = {1906--1919},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.173},
  urldate = {2026-02-05},
  abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
  file = {/home/filipe/Zotero/storage/5X6TXJBX/Maynez et al. - 2020 - On Faithfulness and Factuality in Abstractive Summarization.pdf}
}

@inproceedings{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A Visual Language Model for Few-Shot Learning},
  shorttitle = {Flamingo},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millicah, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  year = 2022,
  month = nov,
  series = {{{NIPS}} '22},
  pages = {23716--23736},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2026-02-10},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  isbn = {978-1-7138-7108-8},
  file = {/home/filipe/Zotero/storage/ABNT2EZ3/Alayrac et al. - 2022 - Flamingo a visual language model for few-shot learning.pdf}
}
