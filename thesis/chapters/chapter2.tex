\chapter{Related Work\label{cha:chapter2}}

\section{Hallucinations \label{sec:rw-hallucinations}}

\subsection{Hallucinations in Natural Language Generation}
The term hallucination in the context of machine learning is commonly defined as "content that is nonsensical or unfaithful to the provided source content" \cite{filippovaControlledHallucinationsLearning2020}. In practice, "hallucination" is used to describe a vast array of different errors with different underlying mechanisms that result in similar symptoms. A few examples of errors commonly called hallucinations include: when language models are consistently wrong because of erroneous training data; when language models "lie" in pursuit of a reward; when a language model's answer is sensitive to irrelevant details such as a random seed (defined as "confabulation") \cite{farquharDetectingHallucinationsLarge2024}. Many more types of domain-specific hallucinations exist, such as object hallucinations and co-occurrence hallucinations common in Vision-Language image captioning \cite{ICLR2024_fc625e83}. The term has expanded to include anything generated by a foundation model that is not based in "fact" or "faithfulness". Generated content is "faithful" if based in source knowledge and "factual" if based in world knowledge. However, many applications treat source input as "fact" making "faithfulness" and "factuality" interchangeable \cite{jiSurveyHallucinationNatural2023}. Thus, hallucinations can be categorized into "intrinsic hallucinations", unfaithful content that contradicts source knowledge, and "extrinsic hallucinations", unfaithful content that cannot be verified by source knowledge but might still be factual \cite{jiSurveyHallucinationNatural2023}.

One of the main causes of hallucinations are data-related problems, such as when target data contains additional information not supported by the source information. This source-reference divergence may be caused by heuristic data collection used to build large datasets. For instance, the authors of WIKIBIO collected and utilized infoboxes from Wikipedia pages as source, and the first sentence of the same Wikipedia page as target. This resulted in source-reference divergence in 62\% of cases, as most WIKIBIO first sentences contain additional information not supported by the corresponding infobox. Source-reference divergence may also be innate in some NLG tasks. For example, certain chat systems enhance their engagingness with relevant facts that are not present in the user input, history or provided knowledge source, leading to inevitable extrinsic hallucinations. Another possible hallucination cause is the presence of duplicates in datasets, which may bias the model into repeating memorized sentences present in duplicated examples. Hallucinations may also be caused by limited exposure to niche domain-specific data or a general lack of source-target pairs, both problems are prevalent in Remote Sensing image-captioning.

The second main cause for hallucinations are training-related issues that affect different components of neural models. Encoders, responsible for compressing input data into meaningful representations, can have defective comprehension abilities or learn wrong correlations in data, increasing the rate of hallucination. Decoders, responsible for transforming encoded representations into output text, can attend to the wrong part of the encoded input or introduce randomness through sampling strategies and increase hallucination probability. Large pre-trained models can also suffer from parametric knowledge bias, this can lead to the model prioritizing parametric knowledge over provided input.

\subsection{Hallucinations in Vision-Language applications}

\subsubsection{Hallucination Types}
In Vision-Language, text is generated based on visual input. Images are a fundamentally different medium to text, and as such, the hallucinations that occur in image captioning are different from those that occur in other NLG tasks. Hallucinations can manifest as "judgment" errors to simple questions and as inaccuracies in image descriptions. Errors can be in different visual semantics, such as objects, attributes, relations and position. % Maybe add an example image (RS would be cool)

\subsubsection{Hallucination Causes}
Object hallucination has been shown to be caused by three primary factors: co-occurrence, uncertainty and text position.

\subsubsection{Hallucination Metrics}

\subsubsection{Hallucination Mitigation}

\section{XXX \label{sec:ch2xxx}}

\begin{table}[htb] \caption{Table example (Scientific Style)} \centering
	\begin{tabular}{llll} % No vertical lines
		\toprule
		Name & Vendor    & Release Year & Platform               \\
		\midrule
		A    & Microsoft & 2000         & Windows                \\
		B    & Yahoo!    & 2003         & Windows, Mac OS        \\
		C    & Apple     & 2005         & Mac OS                 \\
		D    & Google    & 2005         & Windows, Linux, Mac OS \\
		\bottomrule
	\end{tabular}
	\label{tab:enghistory_clean}
\end{table}

\section{YYY \label{sec:ch2yyy}}

\section{ZZZ \label{sec:ch2zzz}}
