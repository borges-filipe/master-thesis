\chapter{Related Work\label{cha:chapter2}}

\section{Hallucinations \label{sec:rw-hallucinations}}

The term hallucination in the context of machine learning is commonly defined as "content that is nonsensical or unfaithful to the provided source content". In practice, "hallucination" is used to describe a vast array of different errors with different underlying mechanisms that result in similar symptoms. A few examples of errors commonly called hallucinations include: when language models are consistently wrong because of erroneous training data; when language models "lie" in pursuit of a reward; when a language model's answer is sensitive to irrelevant details such as a random seed (defined as a "confabulation"). Many more types of domain-specific hallucinations exist, such as object hallucinations and co-occurrence hallucinations common in Vision-Language image captioning. The term has expanded to include anything generated by a foundation model that presents irrelevant or false information. 

\section{XXX \label{sec:ch2xxx}}

\begin{table}[htb] \caption{Table example (Scientific Style)} \centering
	\begin{tabular}{llll} % No vertical lines
		\toprule
		Name & Vendor    & Release Year & Platform               \\
		\midrule
		A    & Microsoft & 2000         & Windows                \\
		B    & Yahoo!    & 2003         & Windows, Mac OS        \\
		C    & Apple     & 2005         & Mac OS                 \\
		D    & Google    & 2005         & Windows, Linux, Mac OS \\
		\bottomrule
	\end{tabular}
	\label{tab:enghistory_clean}
\end{table}

\section{YYY \label{sec:ch2yyy}}

\section{ZZZ \label{sec:ch2zzz}}
