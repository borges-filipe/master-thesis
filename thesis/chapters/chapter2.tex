\chapter{Related Work\label{cha:chapter2}}

\section{Hallucinations \label{sec:rw-hallucinations}}

\subsection{Hallucinations in Natural Language Generation}
The term hallucination in the context of machine learning is commonly defined as "content that is nonsensical or unfaithful to the provided source content" \cite{filippovaControlledHallucinationsLearning2020}. In practice, "hallucination" is used to describe a vast array of different errors with different underlying mechanisms that result in similar symptoms. A few examples of errors commonly called hallucinations include: when language models are consistently wrong because of erroneous training data; when language models "lie" in pursuit of a reward; when a language model's answer is sensitive to irrelevant details such as a random seed (defined as "confabulation") \cite{farquharDetectingHallucinationsLarge2024}. Many more types of domain-specific hallucinations exist, such as object hallucinations and co-occurrence hallucinations common in Vision-Language image captioning \cite{ICLR2024_fc625e83}. The term has expanded to include anything generated by a foundation model that is not based in "fact" or "faithfulness". Generated content is "faithful" if based in source knowledge and "factual" if based in world knowledge. However, many applications treat source input as "fact" making "faithfulness" and "factuality" interchangeable \cite{jiSurveyHallucinationNatural2023}. Thus, hallucinations can be categorized into "intrinsic hallucinations", unfaithful content that contradicts source knowledge, and "extrinsic hallucinations", unfaithful content that cannot be verified by source knowledge but might still be factual \cite{jiSurveyHallucinationNatural2023}.

One of the main causes of hallucinations are data-related problems, such as when target data contains additional information not supported by the source information. This source-reference divergence may be caused by heuristic data collection used to build large datasets. For instance, the authors of WIKIBIO collected and utilized infoboxes from Wikipedia pages as source, and the first sentence of the same Wikipedia page as target. This resulted in source-reference divergence in 62\% of cases, as most WIKIBIO first sentences contain additional information not supported by the corresponding infobox. Source-reference divergence may also be innate in some NLG tasks. For example, certain chat systems enhance their engagingness with relevant facts that are not present in the user input, history or provided knowledge source, leading to inevitable extrinsic hallucinations. Another possible hallucination cause is the presence of duplicates in datasets, which may bias the model into repeating memorized sentences present in duplicated examples. Hallucinations may also be caused by limited exposure to niche domain-specific data or a general lack of source-target pairs, both problems are prevalent in Remote Sensing image-captioning.

The second main cause for hallucinations are training-related issues that affect different components of neural models. Encoders, responsible for compressing input data into meaningful representations, can have defective comprehension abilities or learn wrong correlations in data, increasing the rate of hallucination. Decoders, responsible for transforming encoded representations into output text, can attend to the wrong part of the encoded input or introduce randomness through sampling strategies and increase hallucination probability. Large pre-trained models can also suffer from parametric knowledge bias, this can lead to the model prioritizing parametric knowledge over provided input.

\subsection{Hallucinations in Vision-Language applications}

\subsubsection{Hallucination Types}
In Vision-Language, text is generated based on visual input. Images are a fundamentally different medium to text, and as such, the hallucinations that occur in image captioning are different from those that occur in other NLG tasks. VLM-Hallucinations occur in two different settings: "judgment hallucinations" occur when the response to a user's question contradicts the actual image, "description hallucinations" are failures to faithfully depict the contents of an image. Both types of hallucination can be seen in Figure X. Errors can be in different visual semantics, such as objects, attributes, relations, position and count. Examples of these errors are also depicted in Figure X. Object hallucinations happen when generated text includes objects that do not actually exist in the input image. Attribute hallucinations occur when generated text claims incorrectly assigns a quality, such as color, to an existing object in an image. A relation hallucination is when the relationship between two objects is incorrectly described, for example "the car is to the left of the bicycle" or "the cloud is casting a shadow on the beach". Hallucinations due to count error are when the count of a specific, existing object in an image is wrong, for example, if a generated description states that there are 4 cars in an image when in reality there are only 3.

\subsubsection{Hallucination Causes}

Most of the literature on VLM-Hallucinations has focused on object hallucinations. The two primary causes of object hallucinations in LVLMs are statistical bias and language priors \cite{lengMitigatingObjectHallucinations2024}.

Statistical bias refers to a model's tendency to rely on superficial object correlations and imbalanced distributions found in its vision-language training data. Due to statistical bias, models are more likely to hallucinate objects that are seen often during training. This also leads to co-occurrence hallucinations, models are more likely to hallucinate objects that frequently appear in the same images as other objects. For example, a model might incorrectly hallucinate a "fork" simply because it correctly identified a "dining table" in the image, or hallucinate a "parking lot" because it correctly identified a shopping mall.



Language priors are inherited by LVLMs from their language model component. Since LLMs are pre-trained on vast amounts of text-only data, they are designed to predict the next word based on linguistic probability rather than visual evidence.


For example, Alayrac et al. \cite{alayracFlamingoVisualLanguage2022} show that Large VLMs, when prompted with a question and image pair, hallucinate by producing answers that seem plausible given the text only, but contradict the contents of the given image. Additionally, visual uncertainty, such as unclear or distorted images, can exacerbate the reliance on language priors. Leng et al. \cite{lengMitigatingObjectHallucinations2024} illustrate this with an image of a "black banana". While the LVLM correctly describes the banana as black, it fails to do so when the image is distorted. As the level of visual uncertainty increases, the model increasingly relies on its language priors and favors more conventional banana colors such as "yellow" or "green".

A recent study performed a statistical analysis of key factors underlying object hallucinations: co-occurrence, uncertainty and text position. Uncertainty is the score used to measure a VLM's confidence in generating the next token during decoding. Text position measures how early or late an object appears in generated text. As seen in figure Y, object hallucinations tend to occur among objects that frequently co-occur, objects with a high uncertainty score during decoding and among objects that are generated in the later part of generated text \cite{ICLR2024_fc625e83}. Indeed, high co-occurrence and high uncertainty lead VLMs to hallucination due to over-reliance on statistical bias and language priors. Additionally, models might hallucinate more at the later parts of generated text due to the autoregressive text generation process. While the model starts off by relying on the semantic information present in the image, the information of past hallucinations and increasing uncertainty cause the model to "snowball". Unlike initial halucinations, "snowball hallucinations" occur when the model maintains consistency with past mistakes. \cite{tangSeeingFarClearly2025b}.





% Maybe add multi-object hallucination (when models focus on multiple objects)
% Maybe add examples from visual contrastive decoding techniques

\subsubsection{Hallucination Metrics}

\subsubsection{Hallucination Mitigation}

\section{XXX \label{sec:ch2xxx}}

\begin{table}[htb] \caption{Table example (Scientific Style)} \centering
	\begin{tabular}{llll} % No vertical lines
		\toprule
		Name & Vendor    & Release Year & Platform               \\
		\midrule
		A    & Microsoft & 2000         & Windows                \\
		B    & Yahoo!    & 2003         & Windows, Mac OS        \\
		C    & Apple     & 2005         & Mac OS                 \\
		D    & Google    & 2005         & Windows, Linux, Mac OS \\
		\bottomrule
	\end{tabular}
	\label{tab:enghistory_clean}
\end{table}

\section{YYY \label{sec:ch2yyy}}

\section{ZZZ \label{sec:ch2zzz}}
