\documentclass[oneside, paper=A4, DIV=15]{scrartcl}
\usepackage{graphicx, lipsum}
\usepackage[table, xcdraw]{xcolor}
\usepackage[backend=biber, style=numeric, maxbibnames=99, sorting=none]{
  biblatex
}
\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}

% \usepackage{etoolbox}
% \AtBeginEnvironment{quote}{\par\singlespacing\small}

\addbibresource{references.bib}

\makeatletter
\def\@maketitle{ \noindent
  \begin{minipage}{0.05\textwidth}\includegraphics[width=2cm]{logo.png}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.90\textwidth}\center{\bfseries\@title}\par\medskip
\end{minipage}\vspace{3em}}
\makeatother

\begin{document}
\title{TU Berlin / Faculty IV\\
	Remote Sensing Image Analysis Group\\
	\vspace{1em}
	\huge Proposal for a Master Thesis}

\maketitle

\begin{table}[h!]
	\begin{tabular}{ >{\columncolor[HTML]{FFFFFF}}r >{\columncolor[HTML]{FFFFFF}}l
		}
		{\color[HTML]{000000} Type of thesis/ Line of study:} &
		{\color[HTML]{000000} Master Thesis}                                         \\
		{\color[HTML]{000000} Title of the thesis:}           &
		{\color[HTML]{000000} On Detecting Hallucinations in Vision-Language Model } \\
		{\color[HTML]{000000} }                               &
		{\color[HTML]{000000} Generated Captions for Remote Sensing Imagery}         \\
		{\color[HTML]{000000} Candidate:}                     &
		{\color[HTML]{000000} Fabian Borges, Filipe Alexandre}                       \\
		{\color[HTML]{000000} Matriculation number:}          &
		{\color[HTML]{000000} 414221}                                                \\
		{\color[HTML]{000000} Advisor(s):}                    &
		{\color[HTML]{000000} Golchin, Pegah (Dr.) \& Beese, Marvin (MSc)}           \\
		{\color[HTML]{000000} Supervisor(s):}                 &
		{\color[HTML]{000000} Demir, Begüm (Prof. Dr.)}                              \\
		{\color[HTML]{000000} Planned period:}                &
		{\color[HTML]{000000} Jan. 2026 until June 2026}
	\end{tabular}
\end{table}

\section{Introduction / Scientific Background / Related Work}

% About \(1 / 2\) to a max. of \(1\) page description of the scientific context (including the classification in the literature, projects, ...), of the concrete embedding (e.g. project at RSiM/TU Berlin) and of existing previous work, if any (e.g. results of a student project, predecessor thesis, …). \\

% Cite like this: \cite{huLeveragingHallucinationsReduce2024}

The field of Remote Sensing faces a significant imbalance between the amount of available image data and the amount of labeled data \cite{andersonMeasuringMitigatingHallucinations2025}. In 2023, The European Space Agency's Copernicus satellite constellation generated 6.8 Petabytes of earth-observation data in just 10 months, more than in the 38 years before 2014 (6.8 PB vs 5 PB) \cite{zavrasGAIAGlobalMultimodal2025}. While there is an abundance of image data from satellites and aerial platforms, the availability of high-quality captioned datasets is limited due to the time-consuming and costly nature of manual annotation. Image captions are important for data analysis, informed decision-making and efficient storage and retrieval of data \cite{basakAerialMirageUnmasking}.

Vision-Language Models (VLMs) are capable of generating rich textual descriptions of images, making them a promising solution for automated image captioning in Remote Sensing \cite{andersonMeasuringMitigatingHallucinations2025}. A successful example of this is the GAIA dataset, which used GPT-generated captions to create a large-scale dataset \cite{zavrasGAIAGlobalMultimodal2025}.

However, VLMs still struggle with Remote Sensing images due to complex backgrounds, imbalanced foreground and background pixel-ratios and different perspective from classical Computer Vision images \cite{liInsightAnyInstance2025}, \cite{basakAerialMirageUnmasking}. VLM-generated captions for Remote Sensing are often generic and contain hallucinations \cite{andersonMeasuringMitigatingHallucinations2025}, commonly defined as “content that is nonsensical or unfaithful to the provided source content” \cite{farquharDetectingHallucinationsLarge2024}. Researchers have explored different causes and types of hallucinations, including co-occurrence, uncertainty and multi-object hallucinations \cite{zhouAnalyzingMitigatingObject2024}, \cite{chenMultiObjectHallucinationVision}. Hallucinatory captions are problematic as they can degrade model performance or directly mislead users who depend on accurate image interpretations \cite{zhouAnalyzingMitigatingObject2024}. Hallucination-specific metrics and detection methods have only recently been proposed, showing that the research field is still in its infancy \cite{liEvaluatingObjectHallucination2023}, \cite{zhouAnalyzingMitigatingObject2024}. One of these metrics is POPE, which converts hallucination evaluation into a binary classification task \cite{liEvaluatingObjectHallucination2023}. Different detection methods have also been proposed, including a hallucination reviser trained on artificially created hallucinations \cite{zhouAnalyzingMitigatingObject2024}. Training-free methods also exist, such as Visual Contrastive Decoding, which purposefully induces hallucinations to extract internal model presumptions \cite{lengMitigatingObjectHallucinations2024}, \cite{huLeveragingHallucinationsReduce2024}.

Most research has focused on general Computer Vision images, but some work on hallucinations in Remote Sensing and aerial imaging does exist. For example, LID \cite{basakAerialMirageUnmasking} is a possible benchmark for hallucination detection in drone imaging. Another example is fMoW-mm \cite{tangSeeingFarClearly2025}, a multimodal dataset that incorporates Remote Sensing specific metadata to mitigate hallucinations. However, these works do not explore if existing hallucination detection and mitigation techniques from general Computer Vision can be applied to Remote Sensing. Thus,  significant work remains to explore hallucinations in image-captioning using VLMs and to adapt existing methods to Remote Sensing.

\section{Problem Statement / Goals of the Thesis}

% About \(1\) page description of the concrete problems addressed as the goals of the thesis, planned/expected results, reference to other thesis, paper, etc., if any.

This thesis will confront two problems present in the field of Remote Sensing. The first problem is cost. Manually captioning the high volume of Remote Sensing image data is prohibitively expensive \cite{andersonMeasuringMitigatingHallucinations2025}. The second problem is that the VLMs used for this automation produce captions that contain hallucinations \cite{andersonMeasuringMitigatingHallucinations2025}. Currently, very little research exists on Remote Sensing hallucination detection and mitigation techniques. One example is the use of maps as external data sources to reduce hallucinations in captions \cite{tangSeeingFarClearly2025}. However, more research is needed to understand and address hallucinations in this domain.

These problems lead directly to the goals of this thesis. The first goal is to improve a Remote Sensing dataset that contains VLM-generated captions. This will be done by exploring different methods to detect and remove hallucinations from generated text. Such datasets already exist, such as GAIA, which uses GPT-4o captions and includes images of varying sources and modalities, and Llama3-SSL4EO-S12, containing one million Sentinel-2 samples and Llama3-LLaVA-Next generated captions \cite{zavrasGAIAGlobalMultimodal2025} \cite{marimoVisibleMultispectralVisionLanguage2026}. Removing hallucinations from existing datasets is important as it could increase down-stream task performance and ultimately provide users with more accurate information \cite{zhouAnalyzingMitigatingObject2024}.

The second goal is to create a new benchmark dataset for hallucination correction. One possibility would be to exploit the first goal by collecting all corrected captions (before and after the correction). This dataset would then contain real examples of VLM-hallucinations in Remote Sensing. At the moment of writing this proposal, I am not aware of any hallucination detection benchmark datasets in the field of Remote Sensing. This could be beneficial to explore how well different hallucination detection techniques perform in the Remote Sensing domain.

This thesis will explore different hallucination detection and revision methods and a hallucination detection benchmark (the new dataset containing real labeled hallucinations). These contributions could make existing VLM captions more reliable and provide new tools for future research.

\section{Thesis Approach / Plan of Implementation}

% Methodological and conceptual approach of the thesis and ideas/plans of implementation (about \(1 / 2\) to a max. of \(1\) page).

% Intro

% TODO should I write again why we are correcting VLM generated captions??

This thesis will consist of two phases. The first phase will be an exploration of two training-free hallucination detection methods: Semantic Entropy \cite{farquharDetectingHallucinationsLarge2024} and Visual Contrastive Decoding \cite{lengMitigatingObjectHallucinations2024}. In this phase, the performance of these methods will be evaluated on Remote Sensing datasets with VLM-generated captions. As both methods rely on VLMs, different models will be tested to see how model choice affects detection performance. Detected hallucinations will be collected and revised in order to create a dataset of detected VLM hallucinations in Remote Sensing image captions. In the second phase, the created dataset will be used to train a hallucination reviser similarly to LURE \cite{zhouAnalyzingMitigatingObject2024}.

% TODO add figure illustrating both phases

\subsection{Hallucination Detection Methods}

\subsection{Datasets}
The main task throughout the thesis will be to detect and revise hallucinations in existing  VLM-generated Remote Sensing image captions. Therefore, all datasets used must contain VLM-generated captions. The first dataset to be used will be GAIA \cite{zavrasGAIAGlobalMultimodal2025}, which contains GPT-4o generated captions for a diverse set of Remote Sensing images. The images provided by GAIA come from different sources, span a wide variety of scenes and include multi-modal data (such as multispectral imagery). This variety will offer a solid foundation for exploring hallucination detection methods. Another dataset that will be explored is RSTeller, which exclusively contains aircraft imagery captured over the United States of America sourced from the National Agriculture Imagery Program. All images in RSTeller contain multiple captions generated with different Mixtral VLMs. RSTeller contains both longer descriptive captions and shorter revised versions, which could be useful for evaluating detection methods on captions with different lengths. This will also provide captions generated form a different VLM architecture than GAIA, allowing for a broader evaluation of detection methods. Other Remote Sensing datasets with rich VLM-generated captions also exists, such as fMoW-mm \cite{tangSeeingFarClearly2025}, RS5M \cite{zhangRS5MGeoRSCLIPLarge2024} and ChatEarthNet \cite{yuanChatEarthNetGlobalScaleImageText2024a}. This thesis will primarily focus on GAIA and RSTeller, but other datasets may be explored if time permits.


\subsubsection{Uncertainty Estimation}

Semantic Entropy (SE) and Visual Contrastive Decoding (VCD) are training-free methods to detect hallucinations, both methods are explained in the following paragraphs. Both estimate uncertainty for singular words or ideas that might be present in a generated caption. Descriptions of complex Remote Sensing images often contain multiple facts. To analyze these, an LLM will decompose each caption into individual facts and generate corresponding questions, as described in the original Semantic Entropy paper \cite{farquharDetectingHallucinationsLarge2024}. These questions will serve as the textual input for both detection methods. For example, below is a caption taken from the GAIA dataset and a possible factoid decomposition:

\begin{quote}
	"A plume of glacial dust extending into the Gulf of Alaska, originating from the Copper River Valley. The phenomenon is captured by the VIIRS instrument aboard the Suomi NPP satellite. The glacial flour, rich in iron, is visible as a distinct brownish streak over the ocean, highlighting the interaction between terrestrial and marine environments."

	\begin{enumerate}
		\item A plume of glacial dust extends into the Gulf of Alaska.

		\item The glacial dust originates from the Copper River Valley.

		\item The phenomenon was captured by the Visible Infrared Imaging Radiometer Suite (VIIRS) instrument.

		\item The glacial dust is composed of glacial flour.

		\item The glacial flour is rich in iron.

		\item The dust is visible as a distinct brownish streak over the ocean.
	\end{enumerate}
\end{quote}

\paragraph{Semantic Entropy}
Semantic entropy \cite{farquharDetectingHallucinationsLarge2024}, is used as an entropy-based uncertainty estimator for detecting hallucinations in language models. One of the motivations for this method is that a single idea can be expressed in many ways. To address this, entropy is calculated over meaning rather than specific tokens. This method was chosen as it requires no domain or task specific data and was shown to generalize well across different tasks, thus requires no assumptions about Remote Sensing hallucinations \cite{farquharDetectingHallucinationsLarge2024}. The general flow of this method is as follows: For each question generated from one factual claim, prompt the VLM to generate multiple answers and compute the semantic entropy of the answers. The average semantic entropy across all questions is used as the hallucination score for the original factual claim. The paper introducing Semantic Entropy for hallucination detection only evaluated LLM-generated text outside the domain of Remote Sensing. Thus, the efficacy on VLM-generated image descriptions, including Remote Sensing imagery, remains unexplored.

\paragraph{Visual Contrastive Decoding}
Similarly to the semantic entropy method, Visual Contrastive Decoding (VCD) is a training-free method and requires no assumptions about the nature of Remote Sensing hallucinations \cite{lengMitigatingObjectHallucinations2024}. While Semantic Entropy only works with text-based captions, Visual Contrastive Decoding also works with images. By comparing the output distributions of original and distorted images, VCD reveals internal biases present in VLMs. VCD has proven versatile, mitigating hallucinations across different VLM families. This versatility may be important for Remote Sensing, which uses different VLMs for varied image types (such as multi-spectral) \cite{marimoVisibleMultispectralVisionLanguage2026}. As with  Semantic Entropy, VCD's efficacy on Remote Sensing imagery remains unexplored. The original VCD method utilizes Gaussian noise as the distortion technique. However, different types of distortions, such as different kinds of noise or masking, will be explored to determine which is most effective for Remote Sensing images.

Using both aforementioned methods, a dataset containing hallucinated VLM-generated captions of Remote Sensing images will be created. This dataset could serve as a benchmark for hallucination detection in the field of Remote Sensing. Additionally, the utility as a training dataset for fine-tuning a hallucination reviser will be explored.

\subsubsection{Training a Reviser}
A hallucination reviser will be trained similarly to the method proposed by Zhou et al.\ \cite{zhouAnalyzingMitigatingObject2024}. In the original method, a training dataset is created by artificially generating hallucinatory captions. Instead of this, the dataset created in the first phase of the thesis, containing detected hallucinations, will be used. An LLM will then be fine-tuned on this dataset to correct hallucinations in captions. This will serve to evaluate the utility of the newly created dataset and allow for a comparison between the training-free and trained hallucination detection methods.

\subsubsection{Effect of VLM choice}
%TODO add citations
Both training-free methods rely on VLMs to generate answers to questions derived from factual claims in captions. While both methods show comparable performance across different model families \cite{farquharDetectingHallucinationsLarge2024}, \cite{lengMitigatingObjectHallucinations2024}, no tests were performed in the Remote Sensing domain. Different LVLMs will be tested to see how model choice affects hallucination detection performance. VCD relies on logit distributions, so only LVLMs that provide access to these are viable options. In order to explore different VLM architectures LLaVA-1.5 \cite{liuImprovedBaselinesVisual2024} and Qwen3-VL \cite{baiQwen3VLTechnicalReport2025} will be tested. LLaVA-1.5 employs Vicuna-7B as a language decoder and Qwen3-VL is built on top of Qwen3 LM. GeoChat \cite{kuckrejaGeoChatGroundedLarge2023}, a fine-tuned version of LLaVA-1.5 on Remote Sensing data, will also be evaluated to see if domain-specific fine-tuning improves detection performance.

\subsubsection{Evaluating Hallucination Predictions}
To increase confidence of any hallucination predictions, an LLM-as-a-judge or LLMs-as-a-jury strategy will be employed. Different options will be explored. For example, to confirm a likely hallucination, a VLM is asked a binary question for each suspected object (e.g., ”Is object [X] present in the image?”), similar to the POPE evaluation method \cite{liEvaluatingObjectHallucination2023}. Using multiple VLMs and a voting strategy might improve hallucination detection but also adds complexity, this will be explored throughout the thesis.

\subsection{Evaluation}
One way to evaluate the utility of the revised captions is to fine-tune a model with the original and revised captions separately and compare their captioning performance. If the revised captions lead to better performance, it would indicate that the hallucination removal process was effective. Common evaluation metrics for image captioning, such as BLEU, METEOR, or CIDEr, and hallucination detection metrics such as POPE can also be used directly on the revised captions against ground truth captions to assess improvements in quality.

%----------------

\section{Time Frame}

The thesis will be carried out over a period of six months. The following Gantt chart (Figure \ref{fig:schedule}) outlines the estimated schedule for the thesis:

\begin{figure}[H]
	\includegraphics[width=\linewidth]{schedule.png}
	\caption{Gantt chart showing the planned schedule for the master thesis.}
	\label{fig:schedule}
\end{figure}

% If interesting and enlightening (e.g. cooperation with a company or similar) can be given here a brief overview of the schedule. (can be omitted in case of only 3 or 4 month, does not make so much sense in these short frames).

\newpage
\printbibliography
\end{document}
