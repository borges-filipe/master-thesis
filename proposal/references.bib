@online{andersonMeasuringMitigatingHallucinations2025,
  title       = {Measuring and {{Mitigating Hallucinations}} in {{Vision-Language Dataset Generation}} for {{Remote Sensing}}},
  author      = {Anderson, Madeline and Cha, Miriam and Freeman, William T. and Perron, J. Taylor and Maidel, Nathaniel and Cahoy, Kerri},
  date        = {2025-01-24},
  eprint      = {2501.14905},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2501.14905},
  url         = {http://arxiv.org/abs/2501.14905},
  urldate     = {2025-09-16},
  abstract    = {Vision language models have achieved impressive results across various fields. However, adoption in remote sensing remains limited, largely due to the scarcity of paired image-text data. To bridge this gap, synthetic caption generation has gained interest, traditionally relying on rule-based methods that use metadata or bounding boxes. While these approaches provide some description, they often lack the depth needed to capture complex wide-area scenes. Large language models (LLMs) offer a promising alternative for generating more descriptive captions, yet they can produce generic outputs and are prone to hallucination. In this paper, we propose a new method to enhance vision-language datasets for remote sensing by integrating maps as external data sources, enabling the generation of detailed, context-rich captions. Additionally, we present methods to measure and mitigate hallucinations in LLM-generated text. We introduce fMoW-mm, a multimodal dataset incorporating satellite imagery, maps, metadata, and text annotations. We demonstrate its effectiveness for automatic target recognition in few-shot settings, achieving superior performance compared to other vision-language remote sensing datasets.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/home/filipe/Zotero/storage/YQGPEITD/Anderson et al. - 2025 - Measuring and Mitigating Hallucinations in Vision-Language Dataset Generation for Remote Sensing.pdf;/home/filipe/Zotero/storage/YMIZC7TS/2501.html}
}

@article{basakAerialMirageUnmasking,
  title    = {Aerial {{Mirage}}: {{Unmasking Hallucinations}} in {{Large Vision Language Models}}},
  author   = {Basak, Debolena and Bhatt, Soham and Kanduri, Sahith and Desarkar, Maunendra Sankar},
  abstract = {Drones excel at capturing aerial-view images, especially in human unreachable areas. Automatically interpreting and describing these images enables decision-making easier without the need to review the images extensively. Traditional image captioning models struggle with aerial imagery due to diverse orientations, perspectives, and unclear objects. Integrating the capabilities of Large Vision Language Models (LVLMs) with drone images can improve description utility, benefiting strategic missions like surveillance, search and rescue, etc. However, the lack of image-caption datasets for drone imagery poses a significant challenge for training and evaluating drone image captioning. To address this gap, we contribute the first Aerialview Image Captioning dataset (AeroCaps), containing four captions per image. Another major hurdle for the task is the hallucinatory nature of LVLMs. To this end, we perform the first extensive analysis of hallucinations on aerial imagery by two SOTA LVLMs LLaVA and InstructBLIP on our proposed dataset and VisDrone. We explore the reasons behind such hallucinations. We release the LVLM-generated image captions along with our hallucination-labelled annotations as the Labelled Illusion Dataset (LID) for further research. Additionally, we review how effective advanced LLMs like GPT-4 are in evaluating the degree of hallucinations made by other LVLMs like LLaVA.},
  langid   = {english},
  file     = {/home/filipe/Zotero/storage/EHVYUVU3/Basak et al. - Aerial Mirage Unmasking Hallucinations in Large Vision Language Models.pdf}
}

@article{chenMultiObjectHallucinationVision,
  title    = {Multi-{{Object Hallucination}} in {{Vision Language Models}}},
  author   = {Chen, Xuweiyi and Ma, Ziqiao and Zhang, Xuejun and Xu, Sihan and Qian, Shengyi and Yang, Jianing and Fouhey, David F and Chai, Joyce},
  abstract = {Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1) LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2) The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3) Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.},
  langid   = {english},
  file     = {/home/filipe/Zotero/storage/WT3R7JA5/Chen et al. - Multi-Object Hallucination in Vision Language Models.pdf}
}

@article{farquharDetectingHallucinationsLarge2024,
  title        = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author       = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  date         = {2024-06},
  journaltitle = {Nature},
  volume       = {630},
  number       = {8017},
  pages        = {625--630},
  publisher    = {Nature Publishing Group},
  issn         = {1476-4687},
  doi          = {10.1038/s41586-024-07421-0},
  url          = {https://www.nature.com/articles/s41586-024-07421-0},
  urldate      = {2025-09-18},
  abstract     = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has~been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
  langid       = {english},
  keywords     = {Computer science,Information technology},
  file         = {/home/filipe/Zotero/storage/HIKAMJHU/Farquhar et al. - 2024 - Detecting hallucinations in large language models using semantic entropy.pdf}
}

@online{grattafioriLlama3Herd2024,
  title       = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author      = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and family=Linde, given=Jelmer, prefix=van der, useprefix=false and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and family=Maaten, given=Laurens, prefix=van der, useprefix=false and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and family=Oliveira, given=Luke, prefix=de, useprefix=false and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
  date        = {2024-11-23},
  eprint      = {2407.21783},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2407.21783},
  url         = {http://arxiv.org/abs/2407.21783},
  urldate     = {2025-11-16},
  abstract    = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file        = {/home/filipe/Zotero/storage/RVLNJ9A5/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf;/home/filipe/Zotero/storage/FCEIJXF2/2407.html}
}

@article{gunjalDetectingPreventingHallucinations2024,
  title        = {Detecting and {{Preventing Hallucinations}} in {{Large Vision Language Models}}},
  author       = {Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  date         = {2024-03-24},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume       = {38},
  number       = {16},
  pages        = {18135--18143},
  issn         = {2374-3468},
  doi          = {10.1609/aaai.v38i16.29771},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/29771},
  urldate      = {2025-09-19},
  abstract     = {Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41\% and 55\% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15\% and 57\% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.},
  langid       = {english},
  keywords     = {NLP: Safety and Robustness},
  file         = {/home/filipe/Zotero/storage/49FKPMUZ/Gunjal et al. - 2024 - Detecting and Preventing Hallucinations in Large Vision Language Models.pdf}
}

@online{huLeveragingHallucinationsReduce2024,
  title       = {Leveraging {{Hallucinations}} to {{Reduce Manual Prompt Dependency}} in {{Promptable Segmentation}}},
  author      = {Hu, Jian and Lin, Jiayi and Yan, Junchi and Gong, Shaogang},
  date        = {2024-11-22},
  eprint      = {2408.15205},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2408.15205},
  url         = {http://arxiv.org/abs/2408.15205},
  urldate     = {2025-07-04},
  abstract    = {Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, taskgeneric promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer from hallucinations during reasoning, resulting in inaccurate prompting. While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this work, we utilize hallucinations to mine task-related information from images and verify its accuracy for enhancing precision of the generated prompts. Specifically, we introduce an iterative Prompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a mask generator. The prompt generator uses a multiscale chain of thought prompting, initially exploring hallucinations for extracting extended contextual knowledge on a test image. These hallucinations are then reduced to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. The generated masks iteratively induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, jointly resulting in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code given in https://lwpyh.github.io/ProMaC/.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/home/filipe/Zotero/storage/MYCGZT3E/Hu et al. - 2024 - Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation.pdf}
}

@online{kirillovSegmentAnything2023,
  title       = {Segment {{Anything}}},
  author      = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
  date        = {2023-04-05},
  eprint      = {2304.02643},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2304.02643},
  url         = {http://arxiv.org/abs/2304.02643},
  urldate     = {2025-09-25},
  abstract    = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/W5BSW7IC/Kirillov et al. - 2023 - Segment Anything.pdf;/home/filipe/Zotero/storage/H9DZFCJR/2304.html}
}

@inproceedings{lengMitigatingObjectHallucinations2024,
  title      = {Mitigating {{Object Hallucinations}} in {{Large Vision-Language Models}} through {{Visual Contrastive Decoding}}},
  booktitle  = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author     = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  date       = {2024-06-16},
  pages      = {13872--13882},
  publisher  = {IEEE},
  location   = {Seattle, WA, USA},
  doi        = {10.1109/CVPR52733.2024.01316},
  url        = {https://ieeexplore.ieee.org/document/10657718/},
  urldate    = {2025-10-20},
  abstract   = {Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.},
  eventtitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn       = {979-8-3503-5300-6},
  langid     = {english},
  file       = {/home/filipe/Zotero/storage/6UU8QDRK/Leng et al. - 2024 - Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding.pdf}
}

@online{liEvaluatingObjectHallucination2023,
  title       = {Evaluating {{Object Hallucination}} in {{Large Vision-Language Models}}},
  author      = {Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  date        = {2023-10-26},
  eprint      = {2305.10355},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2305.10355},
  url         = {http://arxiv.org/abs/2305.10355},
  urldate     = {2025-10-24},
  abstract    = {Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file        = {/home/filipe/Zotero/storage/LPDI2VDM/Li et al. - 2023 - Evaluating Object Hallucination in Large Vision-Language Models.pdf;/home/filipe/Zotero/storage/KQUJQ9FU/2305.html}
}

@article{liInsightAnyInstance2025,
  title        = {Insight {{Any Instance}}: {{Promptable Instance Segmentation}} for {{Remote Sensing Images}}},
  shorttitle   = {Insight {{Any Instance}}},
  author       = {Li, Xuexue and Diao, Wenhui and Li, Xinming and Sun, Xian},
  date         = {2025},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume       = {63},
  pages        = {1--15},
  issn         = {1558-0644},
  doi          = {10.1109/TGRS.2025.3543636},
  url          = {https://ieeexplore.ieee.org/abstract/document/10892282/figures},
  urldate      = {2025-09-25},
  abstract     = {Instance segmentation of remote sensing images (RSIs) is an essential task for a wide range of applications such as land planning and intelligent transport. Instance segmentation of RSIs is constantly plagued by the unbalanced ratio of foreground and background and limited instance size. And most of the instance segmentation models are based on deep feature learning and contain operations such as multiple downsampling, which is harmful to instance segmentation of RSIs, and thus the performance is still limited. Inspired by the recent superior performance of prompt learning in visual tasks, we propose a new prompt paradigm to address the above issues. Based on the existing instance segmentation model, first, a local prompt module is designed to mine local prompt information from original local tokens for specific instances; second, a global-to-local prompt module is designed to model the contextual information from the global tokens to the local tokens where the instances are located for specific instances. Finally, a proposal’s area loss function (PAreaLoss) is designed to add a decoupling dimension for proposals on the scale to better exploit the potential of the above two prompt modules. It is worth mentioning that our proposed approach can extend the instance segmentation model to a promptable instance segmentation model, i.e., to segment the instances with the specific boxes’ prompt. The time consumption for each promptable instance segmentation process is only 40 ms. This article evaluates the effectiveness of our proposed approach based on several existing models in four instance segmentation datasets of RSIs, and thorough experiments prove that our proposed approach is effective for addressing the above issues and is a competitive model for instance segmentation of RSIs.},
  keywords     = {Computational modeling,Context modeling,Data models,Feature extraction,Global-to-local,instance segmentation,Instance segmentation,prompt,Proposals,remote sensing,Remote sensing,Representation learning,Transformers,Visualization},
  file         = {/home/filipe/Zotero/storage/SCMHV7CN/Li et al. - 2025 - Insight Any Instance Promptable Instance Segmentation for Remote Sensing Images.pdf}
}

@online{liuSurveyHallucinationLarge2024,
  title       = {A {{Survey}} on {{Hallucination}} in {{Large Vision-Language Models}}},
  author      = {Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
  date        = {2024-05-06},
  eprint      = {2402.00253},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2402.00253},
  url         = {http://arxiv.org/abs/2402.00253},
  urldate     = {2025-10-20},
  abstract    = {Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/LK2F52ZC/Liu et al. - 2024 - A Survey on Hallucination in Large Vision-Language Models.pdf;/home/filipe/Zotero/storage/MRRYVFGQ/2402.html}
}

@article{luoSAMRSISProgressivelyAdapting2024,
  title        = {{{SAM-RSIS}}: {{Progressively Adapting SAM With Box Prompting}} to {{Remote Sensing Image Instance Segmentation}}},
  shorttitle   = {{{SAM-RSIS}}},
  author       = {Luo, Muying and Zhang, Tao and Wei, Shiqing and Ji, Shunping},
  date         = {2024},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume       = {62},
  pages        = {1--14},
  issn         = {1558-0644},
  doi          = {10.1109/TGRS.2024.3460085},
  url          = {https://ieeexplore.ieee.org/document/10680168},
  urldate      = {2025-09-25},
  abstract     = {The recent segment anything model (SAM) trained on massive close-range images has demonstrated impressive performance on general segmentation or specific segmentation tasks with manual prompts. However, the significant domain shift problem between remote sensing and close-range images should be tackled before introducing the pretrained SAM to remote sensing instance segmentation (RSIS). To address this and unlock the potential of SAM in RSIS, this article proposes a novel framework called SAM for remote sensing instance segmentation (SAM-RSIS), which overcomes the problems in a few recent works that only adapt a part of SAM to remote sensing. SAM-RSIS fine-tunes the vision transformer (ViT) backbone and mask decoder of SAM progressively on remote sensing data and uses automatic box prompting to eliminate the need for manual prompting. SAM-RSIS consists of an object detection stage and a mask generation stage. In object detection, we introduce an adapter to adapt knowledge embedded in the pretrained ViT backbone to remote sensing images and then build an object detector. In mask generation, using the detected bounding boxes as prompts, along with two learnable mask output tokens, and the two-layer high-resolution features from the adapter, we fine-tune the mask decoder of SAM to produce high-quality masks. Experimental results on the WHU, WHU-Mix, and NWPU datasets for binary and multiclass RSIS demonstrate the effectiveness and robustness of the proposed method, surpassing various derivative methods of SAM and achieving performance comparable to and even better than the specific state-of-the-art instance segmentation methods.},
  keywords     = {Adaptation models,Decoding,Efficient tuning,foundation model,Image segmentation,instance segmentation,Instance segmentation,Object detection,remote sensing,Remote sensing,segment anything model (SAM),Semantics},
  file         = {/home/filipe/Zotero/storage/9SUXMPS8/Luo et al. - 2024 - SAM-RSIS Progressively Adapting SAM With Box Prompting to Remote Sensing Image Instance Segmentatio.pdf}
}

@inproceedings{marimoVisibleMultispectralVisionLanguage2026,
  title      = {Beyond the~{{Visible}}: {{Multispectral Vision-Language Learning}} for~{{Earth Observation}}},
  shorttitle = {Beyond the~{{Visible}}},
  booktitle  = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author     = {Marimo, Clive Tinashe and Blumenstiel, Benedikt and Nitsche, Maximilian and Jakubik, Johannes and Brunschwiler, Thomas},
  editor     = {Ribeiro, Rita P. and Pfahringer, Bernhard and Japkowicz, Nathalie and Larrañaga, Pedro and Jorge, Alípio M. and Soares, Carlos and Abreu, Pedro H. and Gama, João},
  date       = {2026},
  pages      = {359--375},
  publisher  = {Springer Nature Switzerland},
  location   = {Cham},
  doi        = {10.1007/978-3-032-06106-5_21},
  abstract   = {Vision-language models for Earth observation (EO) typically rely on the visual spectrum of data as the only model input, thus failing to leverage the rich spectral information available in the multispectral channels recorded by satellites. Therefore, we introduce Llama3-MS-CLIP—-the first vision-language model pre-trained with contrastive learning on a large-scale multispectral dataset and report on the performance gains due to the extended spectral range. Furthermore, we present the largest-to-date image-caption dataset for multispectral data, consisting of one million Sentinel-2 samples and corresponding textual descriptions generated using Llama3-LLaVA-Next and Overture Maps data. We develop a scalable captioning pipeline, which is validated by domain experts. We evaluate Llama3-MS-CLIP on multispectral zero-shot image classification and retrieval using three datasets of varying complexity. Our results demonstrate that Llama3-MS-CLIP significantly outperforms other RGB-based approaches, improving classification accuracy by +6.77\% on average and retrieval performance by +4.63\% mAP compared to the second-best model. Our results emphasize the relevance of multispectral vision-language learning. The image-caption dataset, code, and model weights are available at https://github.com/IBM/MS-CLIP.},
  isbn       = {978-3-032-06106-5},
  langid     = {english},
  keywords   = {Earth Observation,Multispectral Data,Vision-Language Model},
  file       = {/home/filipe/Zotero/storage/BPR6NQKQ/Marimo et al. - 2026 - Beyond the Visible Multispectral Vision-Language Learning for Earth Observation.pdf}
}

@online{parkHalLocTokenlevelLocalization2025,
  title       = {{{HalLoc}}: {{Token-level Localization}} of {{Hallucinations}} for {{Vision Language Models}}},
  shorttitle  = {{{HalLoc}}},
  author      = {Park, Eunkyu and Kim, Minyeong and Kim, Gunhee},
  date        = {2025-06-12},
  eprint      = {2506.10286},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2506.10286},
  url         = {http://arxiv.org/abs/2506.10286},
  urldate     = {2025-07-04},
  abstract    = {Hallucinations pose a significant challenge to the reliability of large vision-language models, making their detection essential for ensuring accuracy in critical applications. Current detection methods often rely on computationally intensive models, leading to high latency and resource demands. Their definitive outcomes also fail to account for real-world scenarios where the line between hallucinated and truthful information is unclear. To address these issues, we propose HalLoc, a dataset designed for efficient, probabilistic hallucination detection. It features 150K token-level annotated samples, including hallucination types, across Visual Question Answering (VQA), instruction-following, and image captioning tasks. This dataset facilitates the development of models that detect hallucinations with graded confidence, enabling more informed user interactions. Additionally, we introduce a baseline model trained on HalLoc, offering low-overhead, concurrent hallucination detection during generation. The model can be seamlessly integrated into existing VLMs, improving reliability while preserving efficiency. The prospect of a robust plug-and-play hallucination detection module opens new avenues for enhancing the trustworthiness of vision-language models in realworld applications. The HalLoc dataset and code are publicly available at: https://github.com/dbsltm/ cvpr25\_halloc.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/home/filipe/Zotero/storage/W6BAI7LJ/Park et al. - 2025 - HalLoc Token-level Localization of Hallucinations for Vision Language Models.pdf}
}

@online{sureshNoiseNarrativeTracing2025,
  title       = {From {{Noise}} to {{Narrative}}: {{Tracing}} the {{Origins}} of {{Hallucinations}} in {{Transformers}}},
  shorttitle  = {From {{Noise}} to {{Narrative}}},
  author      = {Suresh, Praneet and Stanley, Jack and Joseph, Sonia and Scimeca, Luca and Bzdok, Danilo},
  date        = {2025-09-08},
  eprint      = {2509.06938},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2509.06938},
  url         = {http://arxiv.org/abs/2509.06938},
  urldate     = {2025-10-20},
  abstract    = {As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/TVMNT7HB/Suresh et al. - 2025 - From Noise to Narrative Tracing the Origins of Hallucinations in Transformers.pdf;/home/filipe/Zotero/storage/L8ZXLSJW/2509.html}
}

@online{tangSeeingFarClearly2025,
  title       = {Seeing {{Far}} and {{Clearly}}: {{Mitigating Hallucinations}} in {{MLLMs}} with {{Attention Causal Decoding}}},
  shorttitle  = {Seeing {{Far}} and {{Clearly}}},
  author      = {Tang, Feilong and Liu, Chengzhi and Xu, Zhongxing and Hu, Ming and Peng, Zelin and Yang, Zhiwei and Su, Jionglong and Lin, Minquan and Peng, Yifan and Cheng, Xuelian and Razzak, Imran and Ge, Zongyuan},
  date        = {2025-06-07},
  eprint      = {2505.16652},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2505.16652},
  url         = {http://arxiv.org/abs/2505.16652},
  urldate     = {2025-07-04},
  abstract    = {Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucinationmitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.},
  langid      = {english},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/9WDBB4YY/Tang et al. - 2025 - Seeing Far and Clearly Mitigating Hallucinations in MLLMs with Attention Causal Decoding.pdf}
}

@online{zavrasGAIAGlobalMultimodal2025,
  title       = {{{GAIA}}: {{A Global}}, {{Multi-modal}}, {{Multi-scale Vision-Language Dataset}} for {{Remote Sensing Image Analysis}}},
  shorttitle  = {{{GAIA}}},
  author      = {Zavras, Angelos and Michail, Dimitrios and Zhu, Xiao Xiang and Demir, Begüm and Papoutsis, Ioannis},
  date        = {2025-02-13},
  eprint      = {2502.09598},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2502.09598},
  url         = {http://arxiv.org/abs/2502.09598},
  urldate     = {2025-10-27},
  abstract    = {The continuous operation of Earth-orbiting satellites generates vast and ever-growing archives of Remote Sensing (RS) images. Natural language presents an intuitive interface for accessing, querying, and interpreting the data from such archives. However, existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA's construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computer Vision and Pattern Recognition},
  file        = {/home/filipe/Zotero/storage/TEGAU77I/Zavras et al. - 2025 - GAIA A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis.pdf;/home/filipe/Zotero/storage/MPSZ4PQJ/2502.html}
}

@online{zhouAnalyzingMitigatingObject2024,
  title       = {Analyzing and {{Mitigating Object Hallucination}} in {{Large Vision-Language Models}}},
  author      = {Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  date        = {2024-03-16},
  eprint      = {2310.00754},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2310.00754},
  url         = {http://arxiv.org/abs/2310.00754},
  urldate     = {2025-10-20},
  abstract    = {Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23\% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/9XE2KCNZ/Zhou et al. - 2024 - Analyzing and Mitigating Object Hallucination in Large Vision-Language Models.pdf;/home/filipe/Zotero/storage/NSFRN7LT/2310.html}
}

@article{sumbulBigEarthNetMMLargeScaleMultimodal2021,
  title        = {{{BigEarthNet-MM}}: {{A Large-Scale}}, {{Multimodal}}, {{Multilabel Benchmark Archive}} for {{Remote Sensing Image Classification}} and {{Retrieval}} [{{Software}} and {{Data Sets}}]},
  shorttitle   = {{{BigEarthNet-MM}}},
  author       = {Sumbul, Gencer and family=Wall, given=Arne, prefix=de, useprefix=true and Kreuziger, Tristan and Marcelino, Filipe and Costa, Hugo and Benevides, Pedro and Caetano, Mário and Demir, Begüm and Markl, Volker},
  date         = {2021-09},
  journaltitle = {IEEE Geoscience and Remote Sensing Magazine},
  volume       = {9},
  number       = {3},
  pages        = {174--180},
  issn         = {2168-6831},
  doi          = {10.1109/MGRS.2021.3089174},
  url          = {https://ieeexplore.ieee.org/document/9552024},
  urldate      = {2025-11-27},
  abstract     = {This article presents the multimodal BigEarthNet (BigEarthNet-MM) benchmark archive consisting of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches to support deep learning (DL) studies in multimodal, multilabel remote sensing (RS) image retrieval and classification. Each pair of patches in BigEarthNet-MM is annotated with multilabels provided by the CORINE Land Cover (CLC) map of 2018 based on its thematically most detailed level-3 class nomenclature. Our initial research demonstrates that some CLC classes are challenging to accurately describe by considering only (single-date) BigEarthNet-MM images. In this article, we also introduce an alternative class nomenclature as an evolution of the original CLC labels to address this problem. This is achieved by interpreting and arranging the CLC level-3 nomenclature based on the properties of BigEarthNet-MM images in a new nomenclature of 19 classes. In our experiments, we show the potential of BigEarthNet-MM for multimodal, multilabel image retrieval and classification problems by considering several state-of-the-art DL models.},
  keywords     = {Benchmark testing,Deep learning,Image retrieval,Multimodal sensors,Remote sensing},
  file         = {/home/filipe/Zotero/storage/6MCFHY78/Sumbul et al. - 2021 - BigEarthNet-MM A Large-Scale, Multimodal, Multilabel Benchmark Archive for Remote Sensing Image Cla.pdf;/home/filipe/Zotero/storage/U9EHNTBX/9552024.html}
}

@online{liuVisualInstructionTuning2023,
  title       = {Visual {{Instruction Tuning}}},
  author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  date        = {2023-12-11},
  eprint      = {2304.08485},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2304.08485},
  url         = {http://arxiv.org/abs/2304.08485},
  urldate     = {2025-12-11},
  abstract    = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/QBR4IKJ7/Liu et al. - 2023 - Visual Instruction Tuning.pdf;/home/filipe/Zotero/storage/YJITK5Y4/2304.html}
}

@online{liuImprovedBaselinesVisual2024,
  title       = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author      = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  date        = {2024-05-15},
  eprint      = {2310.03744},
  eprinttype  = {arXiv},
  eprintclass = {cs},
  doi         = {10.48550/arXiv.2310.03744},
  url         = {http://arxiv.org/abs/2310.03744},
  urldate     = {2025-12-11},
  abstract    = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in \textasciitilde 1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  pubstate    = {prepublished},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file        = {/home/filipe/Zotero/storage/NHZ3Q6UP/Liu et al. - 2024 - Improved Baselines with Visual Instruction Tuning.pdf;/home/filipe/Zotero/storage/DXMZW3ZS/2310.html}
}
